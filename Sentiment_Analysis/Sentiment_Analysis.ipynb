{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "906be1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import emoji\n",
    "import regex\n",
    "import string\n",
    "from datetime import timedelta\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sn\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from unidecode import unidecode as unidec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71a31fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_=\"C:\\\\Users\\\\Alex\\\\Personal-Code\\\\ACE_592_Dating_Sim\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee4f05dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#verification\n",
    "import nltk\n",
    "nltk.download('stopwords');\n",
    "stopwords.words('english');\n",
    "from nltk.corpus import stopwords;\n",
    "stopwords.words('english');\n",
    "print (stopwords.words('english'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b2edb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads the csv files into memory for each dating app.\n",
    "#Data has already been cleaed and all three files share the same columns\n",
    "dfT = pd.read_csv(dir_ + \"Wrangled_Data\\\\clean_Tinder.csv\")\n",
    "dfB = pd.read_csv(dir_ + \"Wrangled_Data\\\\clean_Bumble.csv\")\n",
    "dfH = pd.read_csv(dir_ + \"Wrangled_Data\\\\clean_Hinge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6482154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfT.head(5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b199a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts the time_stamp and date columns to datetime64 types\n",
    "def to_datetime(df):\n",
    "    df['date'] = pd.to_datetime(df.date)\n",
    "    df['time_stamp'] = pd.to_datetime(df['time_stamp'])\n",
    "    df_data = df.loc[df['score'] != 0]\n",
    "    return df_data\n",
    "\n",
    "#converts the clean text to UNI-8 for sentiment analysis\n",
    "# def to_UNI8(df):\n",
    "    \n",
    "\n",
    "dfT = to_datetime(dfT)\n",
    "dfB = to_datetime(dfB)\n",
    "dfH = to_datetime(dfH)\n",
    "\n",
    "dfT = dfT[['userName', 'full_text', 'clean_text', 'score', 'thumbs', 'date', 'time_stamp', 'reply_status', 'reply_content']]\n",
    "dfB = dfB[['userName', 'full_text', 'clean_text', 'score', 'thumbs', 'date', 'time_stamp', 'reply_status', 'reply_content']]\n",
    "dfH = dfH[['userName', 'full_text', 'clean_text', 'score', 'thumbs', 'date', 'time_stamp', 'reply_status', 'reply_content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ebd38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_15200\\2723410026.py:2: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n",
      "  df_T = dfT.loc[dfT['time_stamp'] >= np.datetime64('2018-01-01 00:00:00+00:00')]\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_15200\\2723410026.py:3: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n",
      "  df_B = dfB.loc[dfB['time_stamp'] >= np.datetime64('2018-01-01 00:00:00+00:00')]\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_15200\\2723410026.py:4: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n",
      "  df_H = dfH.loc[dfH['time_stamp'] >= np.datetime64('2018-01-01 00:00:00+00:00')]\n"
     ]
    }
   ],
   "source": [
    "#clips the df's into just the reviews from 2018-2022\n",
    "df_T = dfT.loc[dfT['time_stamp'] >= np.datetime64('2018-01-01 00:00:00+00:00')]\n",
    "df_B = dfB.loc[dfB['time_stamp'] >= np.datetime64('2018-01-01 00:00:00+00:00')]\n",
    "df_H = dfH.loc[dfH['time_stamp'] >= np.datetime64('2018-01-01 00:00:00+00:00')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3c02971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303647, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528e7867",
   "metadata": {},
   "source": [
    "### Let's take the whole dataframe (dropping reviews without comments) and analyze the sentiment of each review. Let's also examine the top 10 most positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc8adff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_T.head(10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db2dd18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userName                 object\n",
       "full_text                object\n",
       "clean_text               object\n",
       "score                     int64\n",
       "thumbs                    int64\n",
       "date             datetime64[ns]\n",
       "time_stamp       datetime64[ns]\n",
       "reply_status               bool\n",
       "reply_content            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_T.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd2e159",
   "metadata": {},
   "source": [
    "### Topic Analysis using unsupervised machine learning: Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc6bb95",
   "metadata": {},
   "source": [
    "### Sentiment Analysis using Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93046b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent(df):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    #takes the clean text and encodes it in utf-8, otherwise Vader will be most unpleased\n",
    "    clean = df['clean_text']\n",
    "    clean = str(clean).encode('utf-8')\n",
    "\n",
    "    #calculates the sentiment of each review and stores the results in the \n",
    "    df1 = df.copy()\n",
    "    df1['comp_sent'] = df['clean_text'].apply(lambda clean:sid.polarity_scores(str(clean))['compound'])\n",
    "    df1['neg_sent'] = df['clean_text'].apply(lambda clean:sid.polarity_scores(str(clean))['neg'])\n",
    "    df1['pos_sent'] = df['clean_text'].apply(lambda clean:sid.polarity_scores(str(clean))['pos'])\n",
    "    df1['neu_sent'] = df['clean_text'].apply(lambda clean:sid.polarity_scores(str(clean))['neu'])\n",
    "    \n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "089d24a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collects the sentiment scores for each of the trimmed dataframes\n",
    "df_T = get_sent(df_T)\n",
    "df_B = get_sent(df_B)\n",
    "df_H = get_sent(df_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5472915",
   "metadata": {},
   "source": [
    "### Let's gather length of review as well and see if that will corrolate with review score\n",
    "#### Then I should calc the correlation coeff for score with each of the sentiments and review length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00b26d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wc(df):\n",
    "    #throws the copy error warning because it annoys me\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    \n",
    "    #drops the columns without text. We won't be using them for the RF since they can't return sentiment scores\n",
    "    df1 = df.dropna(subset = ['clean_text']).reset_index()\n",
    "    #adds a WC column to the dataframe    \n",
    "    df1['word_count'] = np.zeros(len(df1))\n",
    "    \n",
    "    \n",
    "    #Crappy for loop to do this but it's whatever because we have no shortage of time\n",
    "    #runs through the clean text in each row and counts the num of characters. Stores that value in a new df \n",
    "    #which will be merged with the existing df\n",
    "    for i in range(len(df1)):\n",
    "        wc = len(df1['clean_text'][i])\n",
    "        df1['word_count'][i] = wc\n",
    "    \n",
    "    df2 = df1[['userName', 'full_text','clean_text', 'score', 'comp_sent', 'neg_sent', 'pos_sent', 'neu_sent', 'word_count']]\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f9a4d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_T_wc = get_wc(df_T)\n",
    "df_B_wc = get_wc(df_B)\n",
    "df_H_wc = get_wc(df_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92caed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saves the new CSV files as clean data\n",
    "df_T_wc.to_csv(dir_ + \"Wrangled_Data\\\\sent_Tinder.csv\")\n",
    "df_B_wc.to_csv(dir_ + \"Wrangled_Data\\\\sent_Bumble.csv\")\n",
    "df_H_wc.to_csv(dir_ + \"Wrangled_Data\\\\sent_Hinge.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
